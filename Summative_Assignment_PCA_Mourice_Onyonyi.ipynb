{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Morioh/math_for_ml/blob/main/Summative_Assignment_PCA_Mourice_Onyonyi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<center>\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:300/1*mgncZaKaVx9U6OCQu_m8Bg.jpeg\">\n",
        "</center>\n",
        "\n",
        "\n",
        "\n",
        "The goal of PCA is to extract information while reducing the number of features\n",
        "from a dataset by identifying which existing features relate to another. The crux of the algorithm is trying to determine the relationship between existing features, called principal components, and then quantifying how relevant these principal components are. The principal components are used to transform the high dimensional data to a lower dimensional data while preserving as much information. For a principal component to be relevant, it needs to capture information about the features. We can determine the relationships between features using covariance."
      ],
      "metadata": {
        "id": "xyATLU4z1cYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Necessary package\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "UTntK0eUNimH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usecase data\n",
        "data = np.array([\n",
        "    [   1,   2,  -1,   4,  10],\n",
        "    [   3,  -3,  -3,  12, -15],\n",
        "    [   2,   1,  -2,   4,   5],\n",
        "    [   5,   1,  -5,  10,   5],\n",
        "    [   2,   3,  -3,   5,  12],\n",
        "    [   4,   0,  -3,  16,   2],\n",
        "])"
      ],
      "metadata": {
        "id": "qWaiAdz8PyKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Standardize the Data along the Features\n",
        "\n",
        "![image.png](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQLxe5VYCBsaZddkkTZlCY24Yov4JJD4-ArTA&usqp=CAU)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Explain why we need to handle the data on the same scale.\n",
        "\n",
        "**This process ensures that each variable contributes equally to the analysis and prevents variables with larger scales from dominating the result.**"
      ],
      "metadata": {
        "id": "U2U2_Q5ebos3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "standardized_data = scaler.fit_transform(data)\n",
        "standardized_data"
      ],
      "metadata": {
        "id": "JF3eGB7FRC0A",
        "outputId": "497dc93d-cbbb-4c29-daee-533679177ce1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.36438208,  0.70710678,  1.5109662 , -0.99186978,  0.77802924],\n",
              "       [ 0.12403473, -1.94454365, -0.13736056,  0.77145428, -2.06841919],\n",
              "       [-0.62017367,  0.1767767 ,  0.68680282, -0.99186978,  0.20873955],\n",
              "       [ 1.61245155,  0.1767767 , -1.78568733,  0.33062326,  0.20873955],\n",
              "       [-0.62017367,  1.23743687, -0.13736056, -0.77145428,  1.00574511],\n",
              "       [ 0.86824314, -0.35355339, -0.13736056,  1.65311631, -0.13283426]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![cov matrix.webp](https://dmitry.ai/uploads/default/original/1X/9bd2851674ebb55e404cc3ff5e2ffe65b42ff460.png)\n",
        "\n",
        "We use the pair - wise covariance of the different features to determine how they relate to each other. With these covariances, our goal is to group / cluster based on similar patterns. Intuitively, we can relate features if they have similar covariances with other features."
      ],
      "metadata": {
        "id": "7rzoiQ7fMk_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Calculate the Covariance Matrix\n",
        "\n"
      ],
      "metadata": {
        "id": "uuhux3UEcBgw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qn8oujZlK9YR",
        "outputId": "23c3ec69-1f14-4943-98df-f49c2b0f7592"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "covariance_matrix:  [[  2.16666667  -1.06666667  -1.76666667   5.5         -4.36666667]\n",
            " [ -1.06666667   4.26666667   0.46666667  -6.6         19.66666667]\n",
            " [ -1.76666667   0.46666667   1.76666667  -3.3          2.36666667]\n",
            " [  5.5         -6.6         -3.3         24.7        -27.9       ]\n",
            " [ -4.36666667  19.66666667   2.36666667 -27.9         92.56666667]]\n"
          ]
        }
      ],
      "source": [
        "# Calculate the covariance matrix of the standardized data\n",
        "cov_matrix = np.cov(data, rowvar=False)\n",
        "\n",
        "print('covariance_matrix: ', cov_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Eigendecomposition on the Covariance Matrix\n"
      ],
      "metadata": {
        "id": "uXNcG4AFcT08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform eigen decomposition on the covariance matrix\n",
        "\n",
        "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
        "\n",
        "print('eigenvalues: ', eigenvalues)\n",
        "print('eigenvectors: ', eigenvectors)"
      ],
      "metadata": {
        "id": "dmGlQ47tRO5w",
        "outputId": "141d0e05-7967-4a2a-c018-a7156b791e3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eigenvalues:  [1.07224751e+02 1.61823788e+01 1.93173735e+00 1.27579741e-01\n",
            " 2.20003762e-04]\n",
            "eigenvectors:  [[-0.05817655 -0.2631212   0.57237125  0.6292347  -0.45148374]\n",
            " [ 0.19774895 -0.03283879  0.06849106 -0.60720902 -0.7657827 ]\n",
            " [ 0.0328828   0.17887983 -0.75671562  0.45776292 -0.42983171]\n",
            " [-0.33200499 -0.88598416 -0.30234056 -0.11461168  0.01609676]\n",
            " [ 0.91989252 -0.33574235 -0.06059523  0.11259736  0.15724145]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Sort the Principal Components\n",
        "# np.argsort can only provide lowest to highest; use [::-1] to reverse the list"
      ],
      "metadata": {
        "id": "4pWho88fcbJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# np.argsort can only provide lowest to highest; use [::-1] to reverse the list\n",
        "\n",
        "order_of_importance = np.argsort(eigenvalues)[::-1]\n",
        "print ( 'the order of importance is :\\n {}'.format(order_of_importance))\n",
        "\n",
        "# utilize the sort order to sort eigenvalues and eigenvectors\n",
        "sorted_eigenvalues = eigenvalues[order_of_importance]\n",
        "\n",
        "print('\\n\\n sorted eigen values:\\n{}'.format(sorted_eigenvalues))\n",
        "sorted_eigenvectors = eigenvectors[:, order_of_importance]\n",
        "print('\\n\\n The sorted eigen vector matrix is: \\n {}'.format(sorted_eigenvectors))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_znKtzdrTmMg",
        "outputId": "705f305a-e571-4329-a9b1-5bb0c8929e9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the order of importance is :\n",
            " [0 1 2 3 4]\n",
            "\n",
            "\n",
            " sorted eigen values:\n",
            "[1.07224751e+02 1.61823788e+01 1.93173735e+00 1.27579741e-01\n",
            " 2.20003762e-04]\n",
            "\n",
            "\n",
            " The sorted eigen vector matrix is: \n",
            " [[-0.05817655 -0.2631212   0.57237125  0.6292347  -0.45148374]\n",
            " [ 0.19774895 -0.03283879  0.06849106 -0.60720902 -0.7657827 ]\n",
            " [ 0.0328828   0.17887983 -0.75671562  0.45776292 -0.42983171]\n",
            " [-0.33200499 -0.88598416 -0.30234056 -0.11461168  0.01609676]\n",
            " [ 0.91989252 -0.33574235 -0.06059523  0.11259736  0.15724145]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question:\n",
        "\n",
        "1. Why do we order eigen values and eigen vectors?\n",
        "\n",
        "Eigenvectors represent the directions of maximum variance, and eigenvalues represent the magnitude of variance in those directions.\n",
        "\n",
        "2. Is it true we would consider the lowest eigen value compared to the highest? Defend your answer\n",
        "\n",
        "No. The eigenvectors are sorted by their corresponding eigenvalues in descending order. The eigenvectors with the highest eigenvalues are the principal components. By selecting the top principal components (those with the largest eigenvalues), I can reduce the dimensionality of the data while retaining the aspects that contain the most variance.\n"
      ],
      "metadata": {
        "id": "o1nILNGxpTJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You want to see what percentage of information each eigen value holds. You would have print out the percentage of each eigen value using the formula\n",
        "\n",
        "\n",
        "\n",
        "> (sorted eigen values / sum of all sorted eigen values) * 100\n",
        "\n"
      ],
      "metadata": {
        "id": "BWqFGNeNvgEB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OKeMKFRuemB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use sorted_eigenvalues to ensure the explained variances correspond to the eigenvectors\n",
        "\n",
        "#TO DO: Insert code here\n",
        "explained_variance = sorted_eigenvalues / np.sum(sorted_eigenvalues) * 100\n",
        "explained_variance =[\"{:.2f}%\".format(value) for value in explained_variance]\n",
        "print( explained_variance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRMHrffrVOXR",
        "outputId": "588e34f6-786e-4db0-f746-c8772bf8893b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['85.46%', '12.90%', '1.54%', '0.10%', '0.00%']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initialize the number of Principle components then perfrom matrix multiplication with the variable K example k = 3 for 3 priciple components\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> The resulting matrix (with reduced data) = standardized data * vector with columns k\n",
        "\n",
        "See expected output for k = 2\n",
        "\n"
      ],
      "metadata": {
        "id": "qB7H4InbfKx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the number of principal components\n",
        "k = 2\n",
        "\n",
        "# Select the first k eigenvectors\n",
        "eigenvectors_k = sorted_eigenvectors[:, :k]\n",
        "\n",
        "# Transform the original data using the first k principal components\n",
        "reduced_data = np.matmul(standardized_data, eigenvectors_k)\n"
      ],
      "metadata": {
        "id": "C-Rnyq6QVTiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(reduced_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxxBcgQMXe1h",
        "outputId": "4bf0836c-9191-44b2-bb68-d22b04aa0fcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.31389845  1.22362226]\n",
            " [-2.55511419  0.01760889]\n",
            " [ 0.61494463  1.08892909]\n",
            " [-0.03531847 -1.11250845]\n",
            " [ 1.45756867  0.44379893]\n",
            " [-0.7959791  -1.66145072]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(reduced_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNEqS6cuaMSY",
        "outputId": "6e5fc337-8604-4bd2-de3e-1d39efc40354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What are 2 positive effects and 2 negative effects of PCA\n",
        "\n",
        "## Positive Effects of PCA\n",
        "\n",
        "1. **Dimensionality Reduction**: PCA reduces the number of variables in a dataset while retaining most of the information or variability. This simplification can make subsequent analyses more efficient and less resource-intensive, particularly in the context of large datasets.\n",
        "\n",
        "2. **Noise Reduction**: By keeping only the principal components that capture the most variance and discarding the rest, PCA can effectively filter out noise from the data. This can lead to more accurate models since they are trained on the cleaner, more relevant features.\n",
        "\n",
        "## Negative Effects of PCA\n",
        "\n",
        "1. **Loss of Interpretability**: One major drawback of PCA is that the principal components are linear combinations of the original variables and may not have a clear or intuitive interpretation. This makes it difficult to understand the underlying factors driving the patterns in the data.\n",
        "\n",
        "2. **Assumption of Linearity**: PCA assumes that the principal components are linear combinations of the original features, which may not capture complex, non-linear relationships between variables. This can limit its effectiveness in analyzing data where non-linear interactions are significant.\n"
      ],
      "metadata": {
        "id": "UxQ8lTunauMQ"
      }
    }
  ]
}